# Problem

We are building a search bar that lets people do fuzzy search on different Konnect entities (services, routes, nodes). 
You're in charge of creating the backend ingest to power that service built on top of a [CDC stream](https://debezium.io/documentation/reference/stable/connectors/postgresql.html#postgresql-create-events) generated by Debezium

We have provided a jsonl file containing some sample events that can be used to
simulate input stream.


Below are the tasks we want you to complete.

* develop a program that ingests the sample cdc events into a Kafka topic
* develop a program that persists the data from Kafka into Opensearch

# Solution

The package implements the tasks by building a connector framework:

The `konnsearch/connect` module defines a core contract for all connectors and the connection object to sync events between source and sink.

The connectors are implemented under `konnsearch/connectors` package. The following connectors are defined to implement the tasks:

* `JSONSourceConnector`: Acts a source event stream for jsonl events from a file
* `KafkaSourceConnector`: A source connector to stream events from a Kafka topic(s)
* `KafkaSinkConnector`: A sink connector to publish events to a Kafka topic
* `OpensearchSinkConnector`: A sink connector to index events to opensearch

The connector contract allows any source to connect to any sink. Hence events can be moved from one kafka topic to another by simply
using the KafkaSourceConnector as source and KafkaSinkConnector as a sink. New source and sink connectors can be added to support other workflows.

## Getting started

### Setting up the environment

1. Setup a python virtualenv (Make sure you have virtualenv installed) and activate it
```
virtualenv .venv
source .venv/bin/activate
```

2. Install dependencies
```
pip install -r requirements.txt
```

3. Start services (Kafka, opensearch)
```
docker compose up -d
```

The Kafka cluster is accessible locally at `localhost:9092` or `kafka:29092` for services running inside the container network.

Kafka-UI can be accessed at `localhost:8080` to examine the ingested Kafka messages.

Opensearch is accessible locally at `localhost:9200` or `opensearch-node:9200` for services running inside the container network.

### Usage

1. Make sure the virtualenv is activated

2. Run the `kafka_ingest.py` script to publish the cdc events from the jsonl file to Kafka (Topic: `cdc-events`)

3. Run the `opensearch_index.py` script to consume cdc events from Kafka and index them to opensearch (Index: `cdc`)

4. Kafka UI can be used to inspect the messages in the Kafka topic

4. To query the opensearch API and check the indexed events, execute the following search request:
```
curl localhost:9200/cdc/_search
```

### Tests

The project uses pytest for testing. To run the test suite, execute:
```
PYTHONPATH=src pytest
```

### Shutdown

To tear down all services, run:
```
docker compose down
```

## Resources

* `stream.jsonl` contains cdc events that need to be ingested
* `docker-compose.yaml` contains the skeleton services to help you get started
